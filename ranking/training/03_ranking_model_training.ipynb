{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Model Training with XGBoost and MLflow\n",
    "\n",
    "This notebook trains an XGBoost model for ranking movie recommendations.\n",
    "\n",
    "**Goal**: Predict user ratings (1-5 stars) and optimize for ranking quality (NDCG@K)\n",
    "\n",
    "**Approach**:\n",
    "- Start with simple baseline XGBoost model\n",
    "- Track experiments with MLflow\n",
    "- Iterate on hyperparameters\n",
    "- Understand feature importance\n",
    "- Evaluate with NDCG@K and RMSE\n",
    "\n",
    "**Philosophy**: Simple first, then improve. Understand every part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "Load libraries and configure MLflow for experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow tracking\n",
    "mlflow.set_tracking_uri(\"file:///Users/ashishmahuli/Desktop/rec_system/mlruns\")\n",
    "mlflow.set_experiment(\"ranking_model\")\n",
    "\n",
    "print(\"MLflow configured!\")\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment: {mlflow.get_experiment_by_name('ranking_model')}\")\n",
    "print(\"\\nTo view experiments, run: mlflow ui\")\n",
    "print(\"Then open: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Materialized Features\n",
    "\n",
    "Load the pre-computed features from parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load features\nfeatures_dir = Path('../features')\n\ntrain_df = pd.read_parquet(features_dir / 'train_features.parquet')\nval_df = pd.read_parquet(features_dir / 'val_features.parquet')\ntest_df = pd.read_parquet(features_dir / 'test_features.parquet')\n\n# Load metadata\nwith open(features_dir / 'feature_metadata.json', 'r') as f:\n    metadata = json.load(f)\n\nprint(\"Data loaded successfully!\")\nprint(f\"\\nTrain: {train_df.shape}\")\nprint(f\"Val:   {val_df.shape}\")\nprint(f\"Test:  {test_df.shape}\")\nprint(f\"\\nTotal features: {metadata['num_features']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) from target (y)\n",
    "# Exclude: user_id, movie_id, rating (target), timestamp\n",
    "exclude_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['rating']\n",
    "\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df['rating']\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['rating']\n",
    "\n",
    "print(f\"Feature columns ({len(feature_cols)}):\")\n",
    "for group_name, group_features in metadata['feature_groups'].items():\n",
    "    print(f\"  {group_name:15} {len(group_features):2} features\")\n",
    "\n",
    "print(f\"\\nX_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check: verify no missing values\n",
    "missing_train = X_train.isnull().sum().sum()\n",
    "missing_val = X_val.isnull().sum().sum()\n",
    "missing_test = X_test.isnull().sum().sum()\n",
    "\n",
    "print(\"Missing value check:\")\n",
    "print(f\"Train: {missing_train} missing values\")\n",
    "print(f\"Val:   {missing_val} missing values\")\n",
    "print(f\"Test:  {missing_test} missing values\")\n",
    "\n",
    "if missing_train == 0 and missing_val == 0 and missing_test == 0:\n",
    "    print(\"\\n✓ No missing values - data is clean!\")\n",
    "else:\n",
    "    print(\"\\n⚠ Warning: Missing values detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics Implementation\n",
    "\n",
    "Implement metrics for evaluating our ranking model.\n",
    "\n",
    "**Two perspectives**:\n",
    "1. **Rating Prediction**: RMSE/MAE - how accurate are our predicted ratings?\n",
    "2. **Ranking Quality**: NDCG@K - how good is our ranking of items for each user?\n",
    "\n",
    "For recommendation systems, **ranking quality (NDCG@K) is the primary metric**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndcg_at_k(df, predictions, k=10):\n",
    "    \"\"\"\n",
    "    Compute NDCG@K for ranking evaluation.\n",
    "    \n",
    "    NDCG (Normalized Discounted Cumulative Gain) measures ranking quality.\n",
    "    - Range: [0, 1], higher is better\n",
    "    - 1.0 = perfect ranking\n",
    "    - Accounts for position: items at top matter more\n",
    "    - Handles graded relevance: ratings 1-5, not just binary\n",
    "    \n",
    "    Algorithm:\n",
    "    1. For each user, sort items by predicted score (descending)\n",
    "    2. Take top K items\n",
    "    3. Compute DCG using actual ratings as relevance\n",
    "    4. Compute ideal DCG (best possible ranking)\n",
    "    5. NDCG = DCG / IDCG\n",
    "    6. Average across all users\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with user_id and rating (actual)\n",
    "        predictions: Array of predicted ratings\n",
    "        k: Number of top items to consider\n",
    "    \n",
    "    Returns:\n",
    "        Average NDCG@K across all users\n",
    "    \"\"\"\n",
    "    # Add predictions to dataframe\n",
    "    df_eval = df[['user_id', 'rating']].copy()\n",
    "    df_eval['prediction'] = predictions\n",
    "    \n",
    "    ndcg_scores = []\n",
    "    \n",
    "    # Compute NDCG for each user\n",
    "    for user_id, user_df in df_eval.groupby('user_id'):\n",
    "        # Skip users with too few items (less than 2)\n",
    "        if len(user_df) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Sort by prediction (descending) - this is our ranking\n",
    "        user_df_sorted = user_df.sort_values('prediction', ascending=False)\n",
    "        \n",
    "        # Take top K\n",
    "        top_k = user_df_sorted.head(k)\n",
    "        \n",
    "        # Compute DCG@K\n",
    "        # DCG = sum(rel_i / log2(i+1)) for i in 1..K\n",
    "        # rel_i is the actual rating at position i\n",
    "        relevances = top_k['rating'].values\n",
    "        positions = np.arange(1, len(relevances) + 1)\n",
    "        dcg = np.sum(relevances / np.log2(positions + 1))\n",
    "        \n",
    "        # Compute Ideal DCG (IDCG) - best possible ranking\n",
    "        # Sort by actual rating (descending)\n",
    "        ideal_relevances = np.sort(user_df['rating'].values)[::-1][:k]\n",
    "        ideal_positions = np.arange(1, len(ideal_relevances) + 1)\n",
    "        idcg = np.sum(ideal_relevances / np.log2(ideal_positions + 1))\n",
    "        \n",
    "        # NDCG = DCG / IDCG (handle division by zero)\n",
    "        if idcg > 0:\n",
    "            ndcg = dcg / idcg\n",
    "            ndcg_scores.append(ndcg)\n",
    "    \n",
    "    # Return average NDCG across all users\n",
    "    return np.mean(ndcg_scores) if ndcg_scores else 0.0\n",
    "\n",
    "print(\"NDCG@K function defined!\")\n",
    "print(\"\\nWhat is NDCG@K?\")\n",
    "print(\"- Measures how well we rank items for each user\")\n",
    "print(\"- 1.0 = perfect ranking (best items at top)\")\n",
    "print(\"- 0.0 = worst ranking (worst items at top)\")\n",
    "print(\"- Position matters: top-K items weighted more heavily\")\n",
    "print(\"- Standard metric for recommendation systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(y_true, y_pred):\n",
    "    \"\"\"Compute Root Mean Squared Error.\"\"\"\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def compute_mae(y_true, y_pred):\n",
    "    \"\"\"Compute Mean Absolute Error.\"\"\"\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def evaluate_model(model, X, y, df, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate model on a dataset.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all metrics\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Compute metrics\n",
    "    rmse = compute_rmse(y, y_pred)\n",
    "    mae = compute_mae(y, y_pred)\n",
    "    ndcg_10 = compute_ndcg_at_k(df, y_pred, k=10)\n",
    "    ndcg_20 = compute_ndcg_at_k(df, y_pred, k=20)\n",
    "    \n",
    "    metrics = {\n",
    "        f'{prefix}rmse': rmse,\n",
    "        f'{prefix}mae': mae,\n",
    "        f'{prefix}ndcg_10': ndcg_10,\n",
    "        f'{prefix}ndcg_20': ndcg_20\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred\n",
    "\n",
    "print(\"Evaluation functions defined!\")\n",
    "print(\"\\nMetrics we'll track:\")\n",
    "print(\"- RMSE: Rating prediction error (lower is better)\")\n",
    "print(\"- MAE: Mean absolute rating error (lower is better)\")\n",
    "print(\"- NDCG@10: Ranking quality for top 10 items (higher is better)\")\n",
    "print(\"- NDCG@20: Ranking quality for top 20 items (higher is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "def make_ndcg_evaluator(df, k=10):\n    \"\"\"\n    Factory function that creates a custom NDCG evaluator for XGBoost early stopping.\n    \n    This function creates a closure that captures the dataframe containing user_ids\n    and actual ratings. XGBoost's DMatrix doesn't easily support custom grouping\n    by user_id, so we use this workaround to access the full dataframe.\n    \n    Args:\n        df: DataFrame with 'user_id' and 'rating' columns\n        k: Cutoff for NDCG computation (default: 10)\n    \n    Returns:\n        Custom evaluation function compatible with XGBoost's eval_metric parameter\n        \n    Usage:\n        val_ndcg_eval = make_ndcg_evaluator(val_df, k=10)\n        model.fit(X_train, y_train, \n                  eval_set=[(X_val, y_val)], \n                  eval_metric=val_ndcg_eval)\n    \"\"\"\n    def ndcg_eval(y_pred, dtrain):\n        \"\"\"\n        Custom evaluation metric called by XGBoost during training.\n        \n        Args:\n            y_pred: Predicted values (1D numpy array)\n            dtrain: XGBoost DMatrix object (not used, we use captured df instead)\n            \n        Returns:\n            Tuple of (metric_name, metric_value)\n            XGBoost will maximize this value (higher is better for NDCG)\n        \"\"\"\n        # Compute NDCG@K using the captured dataframe\n        ndcg = compute_ndcg_at_k(df, y_pred, k=k)\n        \n        # Return (name, value)\n        # XGBoost maximizes custom metrics by default, which is correct for NDCG\n        return f'ndcg@{k}', ndcg\n    \n    return ndcg_eval\n\nprint(\"Custom NDCG evaluator factory function defined!\")\nprint(\"\\nThis enables NDCG@10-based early stopping:\")\nprint(\"- XGBoost will stop training when validation NDCG@10 stops improving\")\nprint(\"- More principled than RMSE-based early stopping for ranking tasks\")\nprint(\"- Aligns optimization directly with our primary metric\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Custom NDCG Metric for XGBoost Early Stopping\n\nFor hyperparameter tuning, we want XGBoost to stop training based on **NDCG@10** improvement (not RMSE).\n\n**Challenge**: XGBoost's default metrics don't include NDCG@K for regression objectives.\n\n**Solution**: Create a custom evaluation metric using a factory function that captures the validation dataframe.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline XGBoost Model\n",
    "\n",
    "Train a simple baseline model with default parameters.\n",
    "\n",
    "**Goal**: Establish a baseline to compare future experiments against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline parameters\n",
    "baseline_params = {\n",
    "    'objective': 'reg:squarederror',  # Regression task (predict ratings)\n",
    "    'max_depth': 6,                    # Tree depth\n",
    "    'learning_rate': 0.1,              # Step size (eta)\n",
    "    'n_estimators': 100,               # Number of trees\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_jobs': -1                       # Use all CPU cores\n",
    "}\n",
    "\n",
    "print(\"Baseline parameters:\")\n",
    "for key, value in baseline_params.items():\n",
    "    print(f\"  {key:20} {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model with MLflow tracking\n",
    "with mlflow.start_run(run_name=\"baseline_xgboost\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_params(baseline_params)\n",
    "    mlflow.log_param('num_features', len(feature_cols))\n",
    "    mlflow.log_param('train_size', len(X_train))\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training baseline model...\")\n",
    "    baseline_model = xgb.XGBRegressor(**baseline_params)\n",
    "    baseline_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    print(\"✓ Training complete!\")\n",
    "    \n",
    "    # Evaluate on train and val\n",
    "    print(\"\\nEvaluating on train set...\")\n",
    "    train_metrics, _ = evaluate_model(baseline_model, X_train, y_train, train_df, prefix='train_')\n",
    "    \n",
    "    print(\"Evaluating on validation set...\")\n",
    "    val_metrics, _ = evaluate_model(baseline_model, X_val, y_val, val_df, prefix='val_')\n",
    "    \n",
    "    # Log metrics\n",
    "    all_metrics = {**train_metrics, **val_metrics}\n",
    "    mlflow.log_metrics(all_metrics)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.xgboost.log_model(baseline_model, \"model\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BASELINE MODEL RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nTrain metrics:\")\n",
    "    print(f\"  RMSE:     {train_metrics['train_rmse']:.4f}\")\n",
    "    print(f\"  MAE:      {train_metrics['train_mae']:.4f}\")\n",
    "    print(f\"  NDCG@10:  {train_metrics['train_ndcg_10']:.4f}\")\n",
    "    print(f\"  NDCG@20:  {train_metrics['train_ndcg_20']:.4f}\")\n",
    "    \n",
    "    print(\"\\nValidation metrics:\")\n",
    "    print(f\"  RMSE:     {val_metrics['val_rmse']:.4f}\")\n",
    "    print(f\"  MAE:      {val_metrics['val_mae']:.4f}\")\n",
    "    print(f\"  NDCG@10:  {val_metrics['val_ndcg_10']:.4f}\")\n",
    "    print(f\"  NDCG@20:  {val_metrics['val_ndcg_20']:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Store run ID for later reference\n",
    "    baseline_run_id = mlflow.active_run().info.run_id\n",
    "    print(f\"\\nMLflow Run ID: {baseline_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Baseline Results\n",
    "\n",
    "**Expected performance** for MovieLens 1M:\n",
    "- NDCG@10: 0.60-0.75 (good baseline)\n",
    "- RMSE: 0.85-1.0 (on 1-5 scale)\n",
    "\n",
    "**What to check**:\n",
    "- Train vs Val metrics: Large gap suggests overfitting\n",
    "- NDCG@10 vs NDCG@20: Should be similar\n",
    "- RMSE: Should be < 1.5 (if higher, model not learning)\n",
    "\n",
    "**Red flags**:\n",
    "- NDCG@10 < 0.4: Check metric implementation or data leakage\n",
    "- Perfect metrics (1.0): Data leakage\n",
    "- RMSE > 1.5: Model not learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Hyperparameter Tuning (Staged Approach)\n\n**Philosophy**: Tune systematically, not exhaustively.\n\nInstead of grid-searching all parameters (expensive and confusing), we use a staged approach:\n1. **Primary tuning**: Find best learning_rate + tree count (most impactful)\n2. **Diagnostics**: Check if model is over/underfitting\n3. **Secondary tuning**: Only adjust max_depth/colsample if needed\n\n**Why this approach?**\n- Efficient: ≤10 total runs vs. hundreds in full grid search\n- Principled: Reflects how GBDT models are tuned in production\n- Interpretable: Understand impact of each parameter\n- Robust: NDCG@10-based early stopping aligns with our ranking goal\n\n**Parameters we're tuning**:\n- `learning_rate` (primary): Controls step size (0.02 = slow/stable, 0.2 = fast/aggressive)\n- `n_estimators` (primary): Number of trees (selected via early stopping on NDCG@10)\n- `max_depth` (secondary): Tree complexity (only if diagnostics indicate need)\n- `colsample_bytree` (secondary): Feature sampling (only if overfitting)"
  },
  {
   "cell_type": "code",
   "source": "# Stage 1: Primary Tuning - Learning Rate Grid Search\nprint(\"STAGE 1: PRIMARY TUNING (Learning Rate + Early Stopping)\")\nprint(\"=\"*60)\nprint(\"\\nNote: Using RMSE for early stopping, then evaluating with NDCG@10\")\nprint(\"(XGBoost sklearn API doesn't support custom metrics for early stopping)\")\n\nlearning_rates = [0.02, 0.05, 0.1, 0.2]\nstage1_results = []\n\nfor lr in learning_rates:\n    print(f\"\\nTraining with learning_rate={lr}...\")\n    \n    with mlflow.start_run(run_name=f\"stage1_lr_{lr}\"):\n        params = {\n            'max_depth': 6,\n            'learning_rate': lr,\n            'n_estimators': 5000,  # Large upper bound\n            'colsample_bytree': 0.8,\n            'early_stopping_rounds': 50,  # Stop if no improvement for 50 rounds\n            'objective': 'reg:squarederror',\n            'random_state': RANDOM_SEED,\n            'n_jobs': -1\n        }\n        \n        # Log parameters\n        mlflow.log_params(params)\n        mlflow.log_param('stage', 'primary_tuning')\n        \n        # Train with RMSE-based early stopping (XGBoost default)\n        model = xgb.XGBRegressor(**params)\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            verbose=False\n        )\n        \n        # Evaluate with NDCG@10 (our primary metric)\n        val_metrics, _ = evaluate_model(model, X_val, y_val, val_df, prefix='val_')\n        \n        # Log metrics\n        mlflow.log_metrics(val_metrics)\n        mlflow.log_param('best_iteration', model.best_iteration)\n        \n        # Store results\n        stage1_results.append({\n            'lr': lr,\n            'best_iteration': model.best_iteration,\n            'val_ndcg_10': val_metrics['val_ndcg_10'],\n            'val_ndcg_20': val_metrics['val_ndcg_20'],\n            'val_rmse': val_metrics['val_rmse'],\n            'model': model  # Save model for later\n        })\n        \n        print(f\"  Best iteration: {model.best_iteration}\")\n        print(f\"  Val NDCG@10:    {val_metrics['val_ndcg_10']:.4f}\")\n        print(f\"  Val RMSE:       {val_metrics['val_rmse']:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Stage 1 complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Stage 1 Results: Select best learning rate\nprint(\"STAGE 1 RESULTS: Learning Rate Comparison\")\nprint(\"=\"*60)\n\n# Display results table\nresults_df = pd.DataFrame([{\n    'learning_rate': r['lr'],\n    'best_iteration': r['best_iteration'],\n    'val_ndcg_10': r['val_ndcg_10'],\n    'val_rmse': r['val_rmse']\n} for r in stage1_results])\n\nprint(\"\\nResults by Learning Rate:\")\nprint(results_df.to_string(index=False))\n\n# Select best configuration based on NDCG@10\nbest_config = max(stage1_results, key=lambda x: x['val_ndcg_10'])\nbest_lr = best_config['lr']\nbest_n_estimators = best_config['best_iteration']\nbest_model_stage1 = best_config['model']\n\nprint(f\"\\nBest Configuration:\")\nprint(f\"  Learning Rate:     {best_lr}\")\nprint(f\"  Best Iteration:    {best_n_estimators}\")\nprint(f\"  Val NDCG@10:       {best_config['val_ndcg_10']:.4f}\")\nprint(f\"  Val RMSE:          {best_config['val_rmse']:.4f}\")\n\n# Visualize results\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Plot NDCG@10 by learning rate\naxes[0].bar([str(r['lr']) for r in stage1_results], \n            [r['val_ndcg_10'] for r in stage1_results])\naxes[0].set_xlabel('Learning Rate')\naxes[0].set_ylabel('Validation NDCG@10')\naxes[0].set_title('NDCG@10 by Learning Rate')\naxes[0].axhline(y=best_config['val_ndcg_10'], color='r', linestyle='--', alpha=0.5)\n\n# Plot iterations by learning rate\naxes[1].bar([str(r['lr']) for r in stage1_results], \n            [r['best_iteration'] for r in stage1_results])\naxes[1].set_xlabel('Learning Rate')\naxes[1].set_ylabel('Best Iteration (# Trees)')\naxes[1].set_title('Optimal Tree Count by Learning Rate')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nProceeding to Stage 2: Diagnostics...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Interpret Stage 1 Results\n\n**What to look for**:\n- Lower learning rates typically need more trees but often achieve better NDCG@10\n- Higher learning rates train faster but may plateau earlier\n- Best iteration should be well before n_estimators limit (5000)\n  - If best_iteration is near 5000: Model hasn't converged, increase upper bound\n  - If best_iteration < 100: Model converged very quickly\n\n**Expected outcomes for MovieLens**:\n- Best learning_rate: Likely 0.05 or 0.1\n- Best iteration: 200-1000 trees (depends on learning_rate)\n- Validation NDCG@10: 0.70-0.90 (if lower, check feature quality)\n\n**Next**: Proceed to Stage 2 diagnostics to check for over/underfitting.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Stage 2: Diagnostics - Train vs Val Gap Analysis\nprint(\"STAGE 2: DIAGNOSTICS (Train vs Val Gap)\")\nprint(\"=\"*60)\n\n# Evaluate best model on BOTH train and val\nprint(\"\\nEvaluating best model from Stage 1...\")\ntrain_metrics_diag, _ = evaluate_model(best_model_stage1, X_train, y_train, train_df, prefix='train_')\nval_metrics_diag, _ = evaluate_model(best_model_stage1, X_val, y_val, val_df, prefix='val_')\n\n# Compute gaps\nndcg_gap = train_metrics_diag['train_ndcg_10'] - val_metrics_diag['val_ndcg_10']\nrmse_gap = val_metrics_diag['val_rmse'] - train_metrics_diag['train_rmse']\n\nprint(f\"\\nTrain Metrics:\")\nprint(f\"  NDCG@10: {train_metrics_diag['train_ndcg_10']:.4f}\")\nprint(f\"  RMSE:    {train_metrics_diag['train_rmse']:.4f}\")\n\nprint(f\"\\nValidation Metrics:\")\nprint(f\"  NDCG@10: {val_metrics_diag['val_ndcg_10']:.4f}\")\nprint(f\"  RMSE:    {val_metrics_diag['val_rmse']:.4f}\")\n\nprint(f\"\\nGap Analysis:\")\nprint(f\"  NDCG Gap (train - val): {ndcg_gap:.4f}\")\nprint(f\"  RMSE Gap (val - train): {rmse_gap:.4f}\")\n\n# Visualize train vs val\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# NDCG comparison\naxes[0].bar(['Train', 'Val'], \n            [train_metrics_diag['train_ndcg_10'], val_metrics_diag['val_ndcg_10']])\naxes[0].set_ylabel('NDCG@10')\naxes[0].set_title(f'NDCG@10 Comparison (Gap: {ndcg_gap:.4f})')\naxes[0].set_ylim(0.8, 1.0)\n\n# RMSE comparison\naxes[1].bar(['Train', 'Val'], \n            [train_metrics_diag['train_rmse'], val_metrics_diag['val_rmse']])\naxes[1].set_ylabel('RMSE')\naxes[1].set_title(f'RMSE Comparison (Gap: {rmse_gap:.4f})')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Stage 2 Decision: Determine if Stage 3 is needed\nprint(\"STAGE 2 DECISION: Should we proceed to Stage 3?\")\nprint(\"=\"*60)\n\n# Decision rules\nproceed_to_stage3 = False\nstage3_action = None\n\nif ndcg_gap > 0.05:\n    proceed_to_stage3 = True\n    stage3_action = \"REDUCE_COMPLEXITY\"\n    print(f\"\\n⚠ NDCG gap ({ndcg_gap:.4f}) > 0.05: Model is OVERFITTING\")\n    print(\"  Recommendation: Reduce complexity (lower max_depth or colsample_bytree)\")\nelif ndcg_gap < 0.02 and val_metrics_diag['val_ndcg_10'] < 0.70:\n    proceed_to_stage3 = True\n    stage3_action = \"INCREASE_COMPLEXITY\"\n    print(f\"\\n⚠ Small gap but low NDCG@10 ({val_metrics_diag['val_ndcg_10']:.4f}): Model is UNDERFITTING\")\n    print(\"  Recommendation: Increase complexity (higher max_depth)\")\nelse:\n    print(f\"\\n✓ GOOD FIT detected!\")\n    print(f\"  NDCG gap ({ndcg_gap:.4f}) is acceptable\")\n    print(f\"  Val NDCG@10 ({val_metrics_diag['val_ndcg_10']:.4f}) is satisfactory\")\n    print(\"  No further tuning needed - skipping Stage 3\")\n\nprint(f\"\\nProceed to Stage 3: {proceed_to_stage3}\")\nif stage3_action:\n    print(f\"Stage 3 Action: {stage3_action}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Interpret Diagnostics\n\n**NDCG Gap Analysis**:\n- **Gap < 0.02**: Excellent generalization, model is well-regularized\n- **Gap 0.02-0.05**: Normal, acceptable\n- **Gap > 0.05**: Overfitting, model memorizing training data\n\n**RMSE Gap Analysis**:\n- **Gap < 0.05**: Good\n- **Gap 0.05-0.10**: Acceptable\n- **Gap > 0.10**: May be overfitting\n\n**Decision**:\nBased on the gaps above, we either:\n- **Good fit**: Stop here, use current model\n- **Overfitting**: Proceed to Stage 3, reduce complexity (lower max_depth or colsample_bytree)\n- **Underfitting**: Proceed to Stage 3, increase complexity (higher max_depth)\n\n**Note**: If diagnostics indicate \"GOOD FIT\", Stage 3 cells will not run (proceed_to_stage3=False).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Stage 3: Secondary Tuning (Only runs if needed)\nif proceed_to_stage3:\n    print(\"STAGE 3: SECONDARY TUNING\")\n    print(\"=\"*60)\n    \n    stage3_results = []\n    \n    if stage3_action == \"REDUCE_COMPLEXITY\":\n        # Try reducing max_depth\n        print(\"\\nTrying reduced max_depth values...\")\n        depths_to_try = [3, 4, 5]\n    else:  # INCREASE_COMPLEXITY\n        # Try increasing max_depth\n        print(\"\\nTrying increased max_depth values...\")\n        depths_to_try = [8, 9, 12]\n    \n    for depth in depths_to_try:\n        print(f\"\\nTraining with max_depth={depth}...\")\n        \n        with mlflow.start_run(run_name=f\"stage3_depth_{depth}\"):\n            params = {\n                'max_depth': depth,\n                'learning_rate': best_lr,\n                'n_estimators': 5000,\n                'colsample_bytree': 0.8,\n                'early_stopping_rounds': 50,\n                'objective': 'reg:squarederror',\n                'random_state': RANDOM_SEED,\n                'n_jobs': -1\n            }\n            \n            mlflow.log_params(params)\n            mlflow.log_param('stage', 'secondary_tuning')\n            mlflow.log_param('action', stage3_action)\n            \n            model = xgb.XGBRegressor(**params)\n            model.fit(\n                X_train, y_train,\n                eval_set=[(X_val, y_val)],\n                verbose=False\n            )\n            \n            val_metrics_s3, _ = evaluate_model(model, X_val, y_val, val_df, prefix='val_')\n            train_metrics_s3, _ = evaluate_model(model, X_train, y_train, train_df, prefix='train_')\n            \n            mlflow.log_metrics(val_metrics_s3)\n            mlflow.log_param('best_iteration', model.best_iteration)\n            \n            gap = train_metrics_s3['train_ndcg_10'] - val_metrics_s3['val_ndcg_10']\n            \n            stage3_results.append({\n                'depth': depth,\n                'best_iteration': model.best_iteration,\n                'val_ndcg_10': val_metrics_s3['val_ndcg_10'],\n                'val_rmse': val_metrics_s3['val_rmse'],\n                'ndcg_gap': gap,\n                'model': model\n            })\n            \n            print(f\"  Best iteration: {model.best_iteration}\")\n            print(f\"  Val NDCG@10:    {val_metrics_s3['val_ndcg_10']:.4f}\")\n            print(f\"  NDCG Gap:       {gap:.4f}\")\n    \n    # Select best from Stage 3\n    best_stage3 = max(stage3_results, key=lambda x: x['val_ndcg_10'])\n    print(f\"\\nBest Stage 3 Configuration:\")\n    print(f\"  max_depth:   {best_stage3['depth']}\")\n    print(f\"  Val NDCG@10: {best_stage3['val_ndcg_10']:.4f}\")\n    print(f\"  NDCG Gap:    {best_stage3['ndcg_gap']:.4f}\")\n    \n    # Compare Stage 1 vs Stage 3\n    if best_stage3['val_ndcg_10'] > best_config['val_ndcg_10']:\n        print(f\"\\n✓ Stage 3 improved NDCG@10!\")\n        final_best_model = best_stage3['model']\n        final_best_params = {'lr': best_lr, 'depth': best_stage3['depth'], 'n_estimators': best_stage3['best_iteration']}\n        final_best_ndcg = best_stage3['val_ndcg_10']\n    else:\n        print(f\"\\n→ Stage 1 model remains best\")\n        final_best_model = best_model_stage1\n        final_best_params = {'lr': best_lr, 'depth': 6, 'n_estimators': best_n_estimators}\n        final_best_ndcg = best_config['val_ndcg_10']\nelse:\n    print(\"Stage 3 SKIPPED (good fit detected in Stage 2)\")\n    final_best_model = best_model_stage1\n    final_best_params = {'lr': best_lr, 'depth': 6, 'n_estimators': best_n_estimators}\n    final_best_ndcg = best_config['val_ndcg_10']\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Hyperparameter tuning complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Final Tuning Summary\nprint(\"HYPERPARAMETER TUNING SUMMARY\")\nprint(\"=\"*60)\n\nprint(f\"\\nTotal MLflow runs: {4 + (len(stage3_results) if proceed_to_stage3 else 0)} (excluding baseline)\")\n\nprint(f\"\\nFinal Best Model Configuration:\")\nprint(f\"  Learning Rate: {final_best_params['lr']}\")\nprint(f\"  Max Depth:     {final_best_params['depth']}\")\nprint(f\"  N Estimators:  {final_best_params['n_estimators']}\")\nprint(f\"  Colsample:     0.8\")\n\nprint(f\"\\nFinal Validation NDCG@10: {final_best_ndcg:.4f}\")\n\n# Compare to baseline\nbaseline_ndcg = val_metrics['val_ndcg_10']  # From Section 4\nimprovement = final_best_ndcg - baseline_ndcg\npct_improvement = (improvement / baseline_ndcg) * 100\n\nprint(f\"\\nComparison to Baseline:\")\nprint(f\"  Baseline NDCG@10:  {baseline_ndcg:.4f}\")\nprint(f\"  Final NDCG@10:     {final_best_ndcg:.4f}\")\nprint(f\"  Improvement:       {improvement:+.4f} ({pct_improvement:+.1f}%)\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Ready for Section 6: Feature Importance Analysis\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Feature Importance Analysis\n\nAnalyze which features are most important for the ranking model.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Get feature importance from best model\nfeature_importance = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': final_best_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"Top 15 Most Important Features:\")\nprint(\"=\"*60)\nfor i, row in feature_importance.head(15).iterrows():\n    print(f\"{row['feature']:30} {row['importance']:.4f}\")\n\n# Plot top 20 features\nplt.figure(figsize=(10, 8))\ntop_n = 20\nplt.barh(range(top_n), feature_importance.head(top_n)['importance'].values[::-1])\nplt.yticks(range(top_n), feature_importance.head(top_n)['feature'].values[::-1])\nplt.xlabel('Feature Importance')\nplt.title(f'Top {top_n} Feature Importances')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Analyze importance by feature group\ngroup_importance = {}\n\nfor group_name, group_features in metadata['feature_groups'].items():\n    group_total = feature_importance[feature_importance['feature'].isin(group_features)]['importance'].sum()\n    group_importance[group_name] = group_total\n\n# Sort by importance\ngroup_importance_sorted = dict(sorted(group_importance.items(), key=lambda x: x[1], reverse=True))\n\nprint(\"Feature Group Importance:\")\nprint(\"=\"*60)\nfor group, importance in group_importance_sorted.items():\n    pct = importance / sum(group_importance.values()) * 100\n    print(f\"{group:15} {importance:8.4f} ({pct:5.1f}%)\")\n\n# Visualize group importance\nplt.figure(figsize=(10, 6))\nplt.bar(group_importance_sorted.keys(), group_importance_sorted.values())\nplt.xlabel('Feature Group')\nplt.ylabel('Total Importance')\nplt.title('Feature Importance by Group')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n✓ Feature importance analysis complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Model Evaluation & Analysis\n\nDeep dive into model performance and error patterns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Final evaluation on validation set\nval_predictions = final_best_model.predict(X_val)\nerrors = val_predictions - y_val\n\nprint(\"Final Model Validation Performance:\")\nprint(\"=\"*60)\nfinal_val_metrics, _ = evaluate_model(final_best_model, X_val, y_val, val_df, prefix='val_')\nprint(f\"NDCG@10: {final_val_metrics['val_ndcg_10']:.4f}\")\nprint(f\"NDCG@20: {final_val_metrics['val_ndcg_20']:.4f}\")\nprint(f\"RMSE:    {final_val_metrics['val_rmse']:.4f}\")\nprint(f\"MAE:     {final_val_metrics['val_mae']:.4f}\")\n\n# Plot: Actual vs Predicted\nplt.figure(figsize=(10, 6))\nplt.scatter(y_val, val_predictions, alpha=0.1, s=1)\nplt.plot([1, 5], [1, 5], 'r--', label='Perfect predictions')\nplt.xlabel('Actual Rating')\nplt.ylabel('Predicted Rating')\nplt.title('Actual vs Predicted Ratings (Validation Set)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nInterpretation:\")\nprint(\"- Points on diagonal: Perfect predictions\")\nprint(\"- Scatter around diagonal: Prediction error\")\nprint(\"- Check for systematic bias (above/below diagonal)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Analyze errors by actual rating\nerror_by_rating = pd.DataFrame({\n    'actual': y_val,\n    'error': np.abs(errors)\n}).groupby('actual')['error'].agg(['mean', 'std', 'count'])\n\nprint(\"Absolute Error by Actual Rating:\")\nprint(\"=\"*60)\nprint(error_by_rating)\n\nplt.figure(figsize=(10, 6))\nplt.bar(error_by_rating.index, error_by_rating['mean'], yerr=error_by_rating['std'], capsize=5)\nplt.xlabel('Actual Rating')\nplt.ylabel('Mean Absolute Error')\nplt.title('Prediction Error by Actual Rating')\nplt.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nInterpretation:\")\nprint(\"- Check if model struggles with extreme ratings (1 or 5)\")\nprint(\"- Larger errors at extremes are common (regression to mean)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Save Best Model\n\nSave the best model and its configuration for later use.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save best model\nmodel_dir = Path('../models')\nmodel_dir.mkdir(parents=True, exist_ok=True)\n\n# Save model\nmodel_path = model_dir / 'xgboost_tuned.json'\nfinal_best_model.save_model(model_path)\n\n# Save feature columns\nfeature_cols_path = model_dir / 'feature_columns.json'\nwith open(feature_cols_path, 'w') as f:\n    json.dump(feature_cols, f, indent=2)\n\n# Save model info\nmodel_info = {\n    'model_name': 'xgboost_tuned',\n    'created_at': pd.Timestamp.now().isoformat(),\n    'params': {\n        'learning_rate': final_best_params['lr'],\n        'max_depth': final_best_params['depth'],\n        'n_estimators': final_best_params['n_estimators'],\n        'colsample_bytree': 0.8,\n        'objective': 'reg:squarederror'\n    },\n    'metrics': {\n        'val_ndcg_10': float(final_val_metrics['val_ndcg_10']),\n        'val_ndcg_20': float(final_val_metrics['val_ndcg_20']),\n        'val_rmse': float(final_val_metrics['val_rmse']),\n        'val_mae': float(final_val_metrics['val_mae'])\n    },\n    'num_features': len(feature_cols),\n    'tuning_summary': {\n        'stage1_runs': 4,\n        'stage3_runs': len(stage3_results) if proceed_to_stage3 else 0,\n        'total_runs': 4 + (len(stage3_results) if proceed_to_stage3 else 0)\n    }\n}\n\nmodel_info_path = model_dir / 'model_info.json'\nwith open(model_info_path, 'w') as f:\n    json.dump(model_info, f, indent=2)\n\nprint(\"Model saved successfully!\")\nprint(\"=\"*60)\nprint(f\"Model:     {model_path}\")\nprint(f\"Features:  {feature_cols_path}\")\nprint(f\"Info:      {model_info_path}\")\nprint(\"\\nTo load model:\")\nprint(\"  model = xgb.XGBRegressor()\")\nprint(f\"  model.load_model('{model_path}')\")\nprint(\"\\nView experiments in MLflow UI:\")\nprint(\"  mlflow ui\")\nprint(\"  Open: http://127.0.0.1:5000\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}